{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import scipy.signal as signal #signal processing stuff (e.g. filters, hilbert transform, etc.)\n",
    "from scipy.stats import linregress\n",
    "import scipy\n",
    "import struct\n",
    "import os\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.axes_grid1.inset_locator import zoomed_inset_axes\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from mpl_toolkits.axes_grid1.inset_locator import mark_inset\n",
    "import svgutils.transform as sg\n",
    "import sys\n",
    "\n",
    "#nelpy!\n",
    "import nelpy as nel  #recommended import for nelpy\n",
    "import nelpy.plotting as npl  #recommended import for the nelpy plotting library\n",
    "import nelpy.io.trodes as neltro\n",
    "\n",
    "#I saved my online ripple detection filter taps and stuff in a file....gonna import this for all analysis\n",
    "import imp\n",
    "ripple_filtering = imp.load_source('ripple_filtering', '/home/shayok/Documents/Code/RippleDetectionAnalysis/Cavaradossi/DataAnalysisScripts/ripple_filtering.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#paper plot settings\n",
    "sns.set(rc={'figure.figsize': (12, 4),'lines.linewidth': 1, 'font.size': 18, 'axes.labelsize': 16, 'axes.titlesize':18, 'legend.fontsize': 12, 'ytick.labelsize': 12, 'xtick.labelsize': 12 })\n",
    "sns.set_style('white')\n",
    "sns.set_color_codes(palette='colorblind')\n",
    "#plots show up within jupyter for matplotlib\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FS=3000\n",
    "FS_system=30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************Loading LFP Timestamps*****************\n",
      "b'<Start settings>\\n'\n",
      "b'Description: LFP timestamps\\n'\n",
      "b'Byte_order: little endian\\n'\n",
      "b'Original_file: singleChan.rec\\n'\n",
      "b'Clock rate: 30000\\n'\n",
      "b'Decimation: 1\\n'\n",
      "b'Time_offset: 0\\n'\n",
      "b'Fields: <time uint32>\\n'\n",
      "Current file position 185\n",
      "Done!\n",
      "*****************Loading LFP Data*****************\n",
      "b'<Start settings>\\n'\n",
      "b'Description: LFP data for one channel\\n'\n",
      "b'Byte_order: little endian\\n'\n",
      "b'Original_file: singleChan.rec\\n'\n",
      "b'nTrode_ID: 2\\n'\n",
      "b'nTrode_channel: 1\\n'\n",
      "b'Clock rate: 30000\\n'\n",
      "b'Voltage_scaling: 0.195\\n'\n",
      "b'Decimation: 1\\n'\n",
      "b'First_timestamp: 19935\\n'\n",
      "b'Reference: on\\n'\n",
      "b'Low_pass_filter: 400\\n'\n",
      "b'Fields: <voltage int16>\\n'\n",
      "Current file position 294\n",
      "*****************Loading LFP Data*****************\n",
      "b'<Start settings>\\n'\n",
      "b'Description: LFP data for one channel\\n'\n",
      "b'Byte_order: little endian\\n'\n",
      "b'Original_file: singleChan.rec\\n'\n",
      "b'nTrode_ID: 3\\n'\n",
      "b'nTrode_channel: 1\\n'\n",
      "b'Clock rate: 30000\\n'\n",
      "b'Voltage_scaling: 0.195\\n'\n",
      "b'Decimation: 1\\n'\n",
      "b'First_timestamp: 19935\\n'\n",
      "b'Reference: on\\n'\n",
      "b'Low_pass_filter: 400\\n'\n",
      "b'Fields: <voltage int16>\\n'\n",
      "Current file position 294\n",
      "*****************Loading LFP Data*****************\n",
      "b'<Start settings>\\n'\n",
      "b'Description: LFP data for one channel\\n'\n",
      "b'Byte_order: little endian\\n'\n",
      "b'Original_file: singleChan.rec\\n'\n",
      "b'nTrode_ID: 4\\n'\n",
      "b'nTrode_channel: 1\\n'\n",
      "b'Clock rate: 30000\\n'\n",
      "b'Voltage_scaling: 0.195\\n'\n",
      "b'Decimation: 1\\n'\n",
      "b'First_timestamp: 19935\\n'\n",
      "b'Reference: on\\n'\n",
      "b'Low_pass_filter: 400\\n'\n",
      "b'Fields: <voltage int16>\\n'\n",
      "Current file position 294\n",
      "19940\n",
      "Data decimated to match data sent to modules\n"
     ]
    }
   ],
   "source": [
    "filePath = \"/home/shayok/Documents/Code/RippleDetectionAnalysis/Cavaradossi/paperData/singleChan.LFP/singleChan.timestamps.dat\"\n",
    "print(\"*****************Loading LFP Timestamps*****************\")\n",
    "f = open(filePath,'rb')\n",
    "instr = f.readline()\n",
    "while (instr != b'<End settings>\\n') :\n",
    "   print(instr)\n",
    "   instr = f.readline()\n",
    "print('Current file position', f.tell())\n",
    "timeStamps = np.fromfile(f, dtype=np.uint32)\n",
    "timeStampsSeconds = (timeStamps-timeStamps[0])/FS_system\n",
    "print(\"Done!\")\n",
    "\n",
    "filePath = \"/home/shayok/Documents/Code/RippleDetectionAnalysis/Cavaradossi/paperData/singleChan.LFP/singleChan.LFP_nt2ch1.dat\" \n",
    "print(\"*****************Loading LFP Data*****************\")\n",
    "f = open(filePath,'rb')\n",
    "instr = f.readline()\n",
    "while (instr != b'<End settings>\\n') :\n",
    "   print(instr)\n",
    "   instr = f.readline()\n",
    "print('Current file position', f.tell())\n",
    "dataT2 = np.fromfile(f, dtype=np.int16)\n",
    "\n",
    "filePath = \"/home/shayok/Documents/Code/RippleDetectionAnalysis/Cavaradossi/paperData/singleChan.LFP/singleChan.LFP_nt3ch1.dat\"\n",
    "print(\"*****************Loading LFP Data*****************\")\n",
    "f = open(filePath,'rb')\n",
    "instr = f.readline()\n",
    "while (instr != b'<End settings>\\n') :\n",
    "   print(instr)\n",
    "   instr = f.readline()\n",
    "print('Current file position', f.tell())\n",
    "dataT3 = np.fromfile(f, dtype=np.int16)\n",
    "\n",
    "filePath = \"/home/shayok/Documents/Code/RippleDetectionAnalysis/Cavaradossi/paperData/singleChan.LFP/singleChan.LFP_nt4ch1.dat\"\n",
    "print(\"*****************Loading LFP Data*****************\")\n",
    "f = open(filePath,'rb')\n",
    "instr = f.readline()\n",
    "while (instr != b'<End settings>\\n') :\n",
    "   print(instr)\n",
    "   instr = f.readline()\n",
    "print('Current file position', f.tell())\n",
    "dataT4 = np.fromfile(f, dtype=np.int16)\n",
    "\n",
    "#Decimate\n",
    "start = 0\n",
    "while(timeStamps[start]%10 != 0):\n",
    "    start+=1\n",
    "timeStamps = timeStamps[start::10]\n",
    "dataT2 = dataT2[start::10]\n",
    "dataT3 = dataT3[start::10]\n",
    "dataT4 = dataT4[start::10]\n",
    "timeStampsSeconds = timeStampsSeconds[start::10]\n",
    "print(timeStamps[0])\n",
    "print(\"Data decimated to match data sent to modules\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Only run first time for data generation! This takes a while otherwise\n",
    "# smoothed_envelope3, rippleData3 = ripple_filtering.rippleBandFilterSimulated(dataT3, timeStampsSeconds, FS,ripple_filtering.bandpassFilterTaps,ripple_filtering.lowpassFilterTaps)\n",
    "# smoothed_envelope4, rippleData4 = ripple_filtering.rippleBandFilterSimulated(dataT4, timeStampsSeconds, FS,ripple_filtering.bandpassFilterTaps,ripple_filtering.lowpassFilterTaps)\n",
    "# np.savetxt(\"/home/shayok/Documents/Code/RippleDetectionAnalysis/Cavaradossi/paperData/twoChanAnalysis/smoothed_envelope_simulatedT3.out\",smoothed_envelope3,fmt='%10.5f')\n",
    "# np.savetxt(\"/home/shayok/Documents/Code/RippleDetectionAnalysis/Cavaradossi/paperData/twoChanAnalysis/smoothed_envelope_simulatedT4.out\",smoothed_envelope4,fmt='%10.5f')\n",
    "# smoothed_envelope2 = np.loadtxt(\"/home/shayok/Documents/Code/RippleDetectionAnalysis/Cavaradossi/paperData/twoChanAnalysis/smoothed_envelopeT2.out\")\n",
    "# smoothed_envelope3, rippleData3 = ripple_filtering.rippleBandFilter(dataT3, timeStampsSeconds, FS=FS)\n",
    "# smoothed_envelope4, rippleData4 = ripple_filtering.rippleBandFilter(dataT4, timeStampsSeconds, FS=FS)\n",
    "# np.savetxt(\"/home/shayok/Documents/Code/RippleDetectionAnalysis/Cavaradossi/paperData/twoChanAnalysis/smoothed_envelopeT3.out\",smoothed_envelope3,fmt='%10.5f')\n",
    "# np.savetxt(\"/home/shayok/Documents/Code/RippleDetectionAnalysis/Cavaradossi/paperData/twoChanAnalysis/smoothed_envelopeT4.out\",smoothed_envelope4,fmt='%10.5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed(\"/home/shayok/Documents/Code/RippleDetectionAnalysis/Cavaradossi/paperData/figureData/Figure6/smoothed_envelopeT2.npz\",smoothed_envelopeT2=smoothed_envelopeT2)\n",
    "np.savez_compressed(\"/home/shayok/Documents/Code/RippleDetectionAnalysis/Cavaradossi/paperData/figureData/Figure6/smoothed_envelopeT3.npz\",smoothed_envelopeT3=smoothed_envelopeT3)\n",
    "np.savez_compressed(\"/home/shayok/Documents/Code/RippleDetectionAnalysis/Cavaradossi/paperData/figureData/Figure6/smoothed_envelopeT4.npz\",smoothed_envelopeT4=smoothed_envelopeT4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothed_envelopeT2 = np.load(\"/home/shayok/Documents/Code/RippleDetectionAnalysis/Cavaradossi/paperData/figureData/Figure6/smoothed_envelopeT2.npz\")['smoothed_envelopeT2']\n",
    "smoothed_envelopeT3 = np.load(\"/home/shayok/Documents/Code/RippleDetectionAnalysis/Cavaradossi/paperData/figureData/Figure6/smoothed_envelopeT3.npz\")['smoothed_envelopeT3']\n",
    "smoothed_envelopeT4 = np.load(\"/home/shayok/Documents/Code/RippleDetectionAnalysis/Cavaradossi/paperData/figureData/Figure6/smoothed_envelopeT4.npz\")['smoothed_envelopeT4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#set threshold, find threshold crossings, and find ripple bounds\n",
    "SecondaryThresholdSigma = 0\n",
    "thresholdSigma = 3\n",
    "lengthCriteria = 0.015 #15 ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Tetrode 2\n",
    "thresholdT2 = np.mean(smoothed_envelopeT2) + thresholdSigma*np.std(smoothed_envelopeT2)\n",
    "\n",
    "#rippleEvents contains indices of events in smoothed_envelope that meet threshold\n",
    "rippleEventsT2, _ = ripple_filtering.thresholdCrossings(smoothed_envelopeT2, thresholdT2)\n",
    "#remove events that don't meet length criteria\n",
    "rippleEventsT2 = rippleEventsT2[rippleEventsT2[:,1] - rippleEventsT2[:,0] >= np.round(FS * lengthCriteria),:]\n",
    "\n",
    "#find start and end of putative ripple aka when ripple band signal comes back down to mean\n",
    "secondaryThresholdT2 = np.mean(smoothed_envelopeT2) + SecondaryThresholdSigma*np.std(smoothed_envelopeT2)\n",
    "rippleBoundsT2, broaderMaxesT2 = ripple_filtering.thresholdCrossings(smoothed_envelopeT2, secondaryThresholdT2)\n",
    "\n",
    "#find windows that match and where they match aka where in the larger bound window does the smaller bound fit\n",
    "outerBoundaryIndicesT2 = np.searchsorted(rippleBoundsT2[:,0], rippleEventsT2[:,0])\n",
    "outerBoundaryIndicesT2 = outerBoundaryIndicesT2 - 1 #subtract 1 since searchsorted returns index after where it belongs\n",
    "\n",
    "#Find extended boundaries for ripple events by pairing to largere window\n",
    "#   (Note that there may be repeats if the larger window contains multiple > 3SD sections)\n",
    "rippleBoundsT2 = rippleBoundsT2[outerBoundaryIndicesT2,:]\n",
    "rippleMaxesT2 = broaderMaxesT2[outerBoundaryIndicesT2]\n",
    "\n",
    "# Now, since all that we care about are the larger windows, so we should get rid of repeats\n",
    "_, unique_idxT2 = np.unique(rippleBoundsT2[:,0], return_index=True)\n",
    "rippleBoundsT2 = rippleBoundsT2[unique_idxT2,:]\n",
    "rippleMaxesT2 = rippleMaxesT2[unique_idxT2]\n",
    "rippleEventsT2 = rippleEventsT2[unique_idxT2,:]\n",
    "\n",
    "offlineRippleDetectionsT2 = np.zeros(smoothed_envelopeT2.size)\n",
    "for i in range(0, rippleMaxesT2.size):\n",
    "    offlineRippleDetectionsT2[(rippleBoundsT2[i][0]):(rippleBoundsT2[i][1])] = rippleMaxesT2[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Tetrode 3\n",
    "thresholdT3 = np.mean(smoothed_envelopeT3) + thresholdSigma*np.std(smoothed_envelopeT3)\n",
    "\n",
    "#rippleEvents contains indices of events in smoothed_envelope that meet threshold\n",
    "rippleEventsT3, _ = ripple_filtering.thresholdCrossings(smoothed_envelopeT3, thresholdT3)\n",
    "#remove events that don't meet length criteria\n",
    "rippleEventsT3 = rippleEventsT3[rippleEventsT3[:,1] - rippleEventsT3[:,0] >= np.round(FS * lengthCriteria),:]\n",
    "\n",
    "#find start and end of putative ripple aka when ripple band signal comes back down to mean\n",
    "secondaryThresholdT3 = np.mean(smoothed_envelopeT3) + SecondaryThresholdSigma*np.std(smoothed_envelopeT3)\n",
    "rippleBoundsT3, broaderMaxesT3 = ripple_filtering.thresholdCrossings(smoothed_envelopeT3, secondaryThresholdT3)\n",
    "\n",
    "#find windows that match and where they match aka where in the larger bound window does the smaller bound fit\n",
    "outerBoundaryIndicesT3 = np.searchsorted(rippleBoundsT3[:,0], rippleEventsT3[:,0])\n",
    "outerBoundaryIndicesT3 = outerBoundaryIndicesT3- 1 #subtract 1 since searchsorted returns index after where it belongs\n",
    "\n",
    "#Find extended boundaries for ripple events by pairing to largere window\n",
    "#   (Note that there may be repeats if the larger window contains multiple > 3SD sections)\n",
    "rippleBoundsT3 = rippleBoundsT3[outerBoundaryIndicesT3,:]\n",
    "rippleMaxesT3 = broaderMaxesT3[outerBoundaryIndicesT3]\n",
    "\n",
    "# Now, since all that we care about are the larger windows, so we should get rid of repeats\n",
    "_, unique_idxT3 = np.unique(rippleBoundsT3[:,0], return_index=True)\n",
    "rippleBoundsT3 = rippleBoundsT3[unique_idxT3,:]\n",
    "rippleMaxesT3 = rippleMaxesT3[unique_idxT3]\n",
    "rippleEventsT3 = rippleEventsT3[unique_idxT3,:]\n",
    "\n",
    "offlineRippleDetectionsT3 = np.zeros(smoothed_envelopeT3.size)\n",
    "for i in range(0, rippleMaxesT3.size):\n",
    "    offlineRippleDetectionsT3[(rippleBoundsT3[i][0]):(rippleBoundsT3[i][1])] = rippleMaxesT3[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt(\"/home/shayok/Documents/Code/RippleDetectionAnalysis/Cavaradossi/paperData/twoChanAnalysis/rippleBoundsStartT2.out\",rippleBoundsT2[:,0],fmt='%10.5f')\n",
    "np.savetxt(\"/home/shayok/Documents/Code/RippleDetectionAnalysis/Cavaradossi/paperData/twoChanAnalysis/rippleBoundsEndT2.out\",rippleBoundsT2[:,1],fmt='%10.5f')\n",
    "np.savetxt(\"/home/shayok/Documents/Code/RippleDetectionAnalysis/Cavaradossi/paperData/twoChanAnalysis/rippleBoundsStartT3.out\",rippleBoundsT3[:,0],fmt='%10.5f')\n",
    "np.savetxt(\"/home/shayok/Documents/Code/RippleDetectionAnalysis/Cavaradossi/paperData/twoChanAnalysis/rippleBoundsEndT3.out\",rippleBoundsT3[:,1],fmt='%10.5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#merge t2 and t3 detections for canonical ripple\n",
    "#merge t2 and t3 detections for canonical ripple\n",
    "#ripples are defined as intersection with two channels. start is the earliest start time out of the two channnels\n",
    "#end is the latter end of the two channels. This should give a conservative detection latency estimate while\n",
    "#encapsulating the entire ripple event across channels\n",
    "rippleBounds = []\n",
    "for i in range(0, rippleMaxesT2.size):\n",
    "    moo = np.squeeze(np.where(offlineRippleDetectionsT3[rippleBoundsT2[i][0]:rippleBoundsT2[i][1]] > 0))\n",
    "    if(moo.size > 0):\n",
    "        if(offlineRippleDetectionsT3[rippleBoundsT2[i][0]] > 0):\n",
    "            rippleBounds.append([rippleBoundsT2[i][0], rippleBoundsT2[i][1]])\n",
    "        else:\n",
    "            if(moo.size == 1): \n",
    "                index = np.squeeze(np.where(moo+rippleBoundsT2[i][0] == rippleBoundsT3[:,0]))\n",
    "                rippleBounds.append([rippleBoundsT3[index][0],rippleBoundsT3[index][1]])\n",
    "            else:\n",
    "                index = np.squeeze(np.where(moo[0]+rippleBoundsT2[i][0] == rippleBoundsT3[:,0]))\n",
    "                rippleBounds.append([rippleBoundsT3[index][0],rippleBoundsT3[index][1]])\n",
    "\n",
    "rippleBounds = np.asarray(rippleBounds)\n",
    "\n",
    "#Merge union of ripple detections\n",
    "offlineRippleDetections = np.zeros(smoothed_envelopeT3.size)\n",
    "for i in range(0, rippleBounds.shape[0]):\n",
    "    offlineRippleDetections[(rippleBounds[i][0]):(rippleBounds[i][1])] = rippleMaxesT2[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 61741,  62979],\n",
       "       [ 76214,  76842],\n",
       "       [ 93610,  94898],\n",
       "       [102885, 103749],\n",
       "       [105662, 106174],\n",
       "       [110621, 110955],\n",
       "       [111693, 111937],\n",
       "       [112006, 112493],\n",
       "       [112712, 112886],\n",
       "       [125374, 126217]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rippleBounds[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "The comment in cell 20 can easily be implemented in python but I just don't want to think and would rather use a nested loop...so it's gonna be done within the C++ functions voting two channel function! It just makes life easier. I think I have code that does the merging with `nelpy` but this is faster for me to prototype :)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Call C++ analysis function\n",
    "Compile with below! Note: I just like having all warnings shown really no reason to compile with those flags<br>\n",
    "`g++ -o votingTwoOfThree -Wall -pedantic -std=c++11 -pthread voting2of3.cpp`<br> \n",
    "or <br>\n",
    "`g++ -o votingTwoOfTwo -Wall -pedantic -std=c++11 -pthread voting2of2.cpp` <--- only one used in the paper<br>\n",
    "*make sure single channel is compiled as well with below! Run it only after the two channel begins i*<br>\n",
    "`g++ -o twoChanDefnSingleChanAnalysis -Wall -pedantic -std=c++11 -pthread singleChanAnalysis_twoChanDefn.cpp`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "The code here is changed to compile metrics for each one of these. File names have been changed for convenience...compile appropriate .cpp file. The file structure will need to be changed if anyone other than me is running this code\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
